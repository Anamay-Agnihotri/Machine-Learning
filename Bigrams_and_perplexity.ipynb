{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <h1> CS 401: Natural Language Processing</h1></center>\n",
    "### <center> <h1> Project 2 </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize # implicitly calls punkt\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordCount(words):\n",
    "    \"\"\"\n",
    "        Helper function that takes the words from \n",
    "        the data and returns frequency counters for \n",
    "        each word as a dictionary.\n",
    "        Parameters:\n",
    "             words: list of all words in data\n",
    "        Return value:\n",
    "            dictionary of frequencies\n",
    "    \"\"\"\n",
    "    worder = {}\n",
    "    for word in words:\n",
    "        if word not in worder:\n",
    "            worder[word] = 0\n",
    "        worder[word] += 1\n",
    "    return worder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBigrams(filename):\n",
    "    \"\"\"\n",
    "        Takes the filename and returns a tuple\n",
    "        of bigram and word frequencies.\n",
    "        Parameters:\n",
    "        filename: textfile\n",
    "        Return Value:\n",
    "            Tuple of bigram and word freqs. \n",
    "    \"\"\"\n",
    "    text1 = open(filename, \"r\")\n",
    "    data1 = text1.read()\n",
    "    bigram_freq = {}\n",
    "    wordlist = []\n",
    "    sent_list = sent_tokenize(data1.lower()) # normalizes text to all lowercase\n",
    "    for sent in sent_list:\n",
    "        words = word_tokenize(sent.strip(),preserve_line=False)\n",
    "        words.append('</s>')\n",
    "        wprev = '<s>'\n",
    "        for w in words:\n",
    "            wordlist.append(w)\n",
    "            bigram = (wprev, w)\n",
    "            if bigram not in bigram_freq:\n",
    "                bigram_freq[bigram] = 0\n",
    "            bigram_freq[bigram] += 1\n",
    "            wprev = w\n",
    "    wordlist.append('<s>')\n",
    "    freqlist = wordCount(wordlist)\n",
    "    return (bigram_freq,freqlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bigrams,worder = getBigrams(\"TheUsualSuspects.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BigramDict(grams,diction,word,worder):\n",
    "    \"\"\"\n",
    "        Function to create bigram matrices\n",
    "        for every unique word in data. Includes\n",
    "        zero probability cases.\n",
    "        Parameters:\n",
    "            grams: bigrams\n",
    "            diction: dictionary to store bigram frequencies\n",
    "            word: unique word in data\n",
    "            worder: word dictionary\n",
    "        Return value:\n",
    "            Updated `diction`\n",
    "    \"\"\"\n",
    "    dictionary = {}\n",
    "    for each in grams:\n",
    "        if each[0] == word:\n",
    "            dictionary[each[1]] = (grams[each]/worder[word])\n",
    "        if each[0] not in dictionary:\n",
    "            dictionary[each[0]] = (0/worder[word])\n",
    "        diction[word] = dictionary\n",
    "    return diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diction = {}\n",
    "for word in worder:\n",
    "    BigramDict(Bigrams,diction,word,worder) # will take a min or so to run. Runtime (O(n^2)) over all words (3874) in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#diction['the'] # includes zero probability cases too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def sorter(diction,word):\n",
    "    \"\"\"\n",
    "        Function to sort probabilities stored in \n",
    "        dictionary.\n",
    "        Parameters:\n",
    "            diction: unsorted dictionary\n",
    "            word: word in dictionary\n",
    "        Return Value:\n",
    "            sorted dictionary\n",
    "    \"\"\"\n",
    "    x = diction[word]\n",
    "    sorted_x = list(sorted(x.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    for each in sorted_x:\n",
    "        if each[0] in string.punctuation: # not sure why this doesn't work\n",
    "            sorted_x.remove(each)\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Finaldict = {}\n",
    "for word in diction:\n",
    "    Finaldict[word] = sorter(diction,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finaldict['the'] # sorted dictionary for every unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word (only lowercase): 0\n",
      "Word not in corpus. Try again.\n",
      "Ok, ending\n"
     ]
    }
   ],
   "source": [
    "# would recommend starting with 'my'\n",
    "import time\n",
    "stringer = \"\"\n",
    "start_word = input('First word (only lowercase): ')\n",
    "if start_word in diction:\n",
    "    word = start_word\n",
    "    index = 1\n",
    "    while index != 0:\n",
    "        values = Finaldict[word]\n",
    "        for each in values:\n",
    "            if each[0] in string.punctuation:\n",
    "                values.remove(each)\n",
    "        stringer += word + \" \"\n",
    "        print(stringer)\n",
    "        index = int(input(\"Choose: 1 %s, 2 %s, 3 %s: \"%(values[0][0],values[1][0],values[2][0])))\n",
    "        if index > 3:\n",
    "            print(\"Choice not valid. Try again.\")\n",
    "            break\n",
    "        word = values[index-1][0]\n",
    "else:\n",
    "    print(\"Word not in corpus. Try again.\")\n",
    "    pass\n",
    "print('Ok, ending')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def getBigramsSmoothed(filename,cutoff):\n",
    "    \"\"\"\n",
    "        Function to create bigram probabilities\n",
    "        with unknown words.\n",
    "        Parameters:\n",
    "            filename: text file\n",
    "            cutoff: frequency cutoff for UNK\n",
    "        Return Value:\n",
    "            Tuple of bigram probs. and word freqs.\n",
    "    \"\"\"\n",
    "    text1 = open(filename, \"r\")\n",
    "    data1 = text1.read()\n",
    "    data1 = data1.lower()\n",
    "    bigram_freq = {}\n",
    "    wordlist = []\n",
    "    p,freqlist = getBigrams(filename) #calls getBigrams to get word frequencies\n",
    "    for word in freqlist:\n",
    "        if freqlist[word] <= cutoff and len(word)>=2: \n",
    "            strings = r\"\\b\"+word+r\"\\b\"\n",
    "            data1 = re.sub(strings,\"UNK\",data1) # makes sure only whole words are replaced\n",
    "    sent_list = sent_tokenize(data1)\n",
    "    for sent in sent_list:\n",
    "        words = word_tokenize(sent.strip(),preserve_line=False)\n",
    "        words.append('</s>')\n",
    "        wprev = '<s>'\n",
    "        for w in words:\n",
    "            wordlist.append(w)\n",
    "            bigram = (wprev, w)\n",
    "            if bigram not in bigram_freq:\n",
    "                bigram_freq[bigram] = 0\n",
    "            bigram_freq[bigram] += 1\n",
    "            wprev = w\n",
    "    wordlist.append('<s>')\n",
    "    freqlist2 = wordCount(wordlist)\n",
    "    return (bigram_freq,freqlist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bigrams2,worder2 = getBigramsSmoothed(\"TheUsualSuspects.txt\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BigramDictSmoother(See,diction,word,worder):\n",
    "    \"\"\"\n",
    "        Function to implement Add-one smoothing\n",
    "        to bigram probabilities.\n",
    "        Parameters:\n",
    "            See: Bigrams dictionary\n",
    "            diction: word probabilities\n",
    "            word: unique word\n",
    "            worder: word dictionary\n",
    "        Return Value:\n",
    "            Updated `diction2`\n",
    "    \"\"\"\n",
    "    dictionary = {}\n",
    "    for each in See:\n",
    "        if each[0] == word:\n",
    "            dictionary[each[1]] = ((See[each]+1)/(worder[word]+len(worder)))\n",
    "        if each[0] not in dictionary:\n",
    "            dictionary[each[0]] = ((0+1)/(worder[word]+len(worder)))\n",
    "        diction[word] = dictionary\n",
    "    return diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diction2 = {}\n",
    "for word in worder2:\n",
    "    BigramDictSmoother(Bigrams2,diction2,word,worder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001806684733514002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#diction2['UNK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def GenPerplexity(filename,worder):\n",
    "    \"\"\"\n",
    "        Function to return bigram\n",
    "        frequency dict. of test data.\n",
    "        Parameters:\n",
    "            filename: text file\n",
    "            worder: word dict. of train data\n",
    "        Return Value:\n",
    "            bigrams\n",
    "    \"\"\"\n",
    "    text2 = open(filename, \"r\")\n",
    "    data2 = text2.read()\n",
    "    data2 = data2.lower()\n",
    "    bigram_freq = {}\n",
    "    wordlist2 = []\n",
    "    \n",
    "    # tokenizing words in new text\n",
    "    sent_list = sent_tokenize(data2)\n",
    "    for sent in sent_list:\n",
    "        words = word_tokenize(sent.strip(),preserve_line=False)\n",
    "        for w in words:\n",
    "            wordlist2.append(w)\n",
    "            \n",
    "    # replacing unseen words in new text with 'UNK'\n",
    "    FullList = wordCount(wordlist2)\n",
    "    for word in FullList:\n",
    "        if word not in worder:\n",
    "            strings = r\"\\b\"+word+r\"\\b\"\n",
    "            data2 = re.sub(strings,\"UNK\",data2)\n",
    "            \n",
    "    # retokenizing new text to compare bigrams\n",
    "    wordlist2 = []\n",
    "    sent_list = sent_tokenize(data2)\n",
    "    for sent in sent_list:\n",
    "        words = word_tokenize(sent.strip(),preserve_line=False)\n",
    "        words.append('</s>')\n",
    "        wprev = '<s>'\n",
    "        for w in words:\n",
    "            wordlist2.append(w)\n",
    "            bigram = (wprev, w)\n",
    "            if bigram not in bigram_freq:\n",
    "                bigram_freq[bigram] = 0\n",
    "            bigram_freq[bigram] += 1\n",
    "            wprev = w\n",
    "    wordlist2.append('<s>')\n",
    "    FullList = wordCount(wordlist2)\n",
    "    return(bigram_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_freq = GenPerplexity(\"Script_ShiningThe.txt\",worder) #10-15 sec runtime. (O(n^2))\n",
    "Commons = dict(set(Bigrams2.keys()).intersection(set(bigram_freq.keys())))\n",
    "Endbigram = {}\n",
    "for each in Commons:\n",
    "    if each in diction2:\n",
    "        if Commons[each] in diction2[each]:\n",
    "            bi = (each,Commons[each])\n",
    "            Endbigram[bi] = diction2[each][Commons[each]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.850814479065267"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logsum = 0\n",
    "for each in Endbigram:\n",
    "    Logsum = Logsum + math.log(Endbigram[each], 2)\n",
    "Perplexity = -Logsum/len(Commons)\n",
    "Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The Shining script - 8.850814479065267\n",
    "# The Heist script - 8.932796821044693\n",
    "# Ranger Boys book - 9.11441064829222"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Comparing the perplexities of The Heist to The Usual Suspects, both Kevin Spacey movies about robbery, it was reassuring to see it have a lower perplexity score (i.e. higher probability) than Ranger Boys, a book for children. The Shining, being in the murder mystery, thriller genre, also has a relatively low perplexity score. However, it is important to note that the training data was a script too, which could have biased the bigrams to be more similar to other scripts than to a book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
